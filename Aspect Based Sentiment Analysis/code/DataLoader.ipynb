{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "from collections import Counter\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import xml.etree.ElementTree as ET\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reuired functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_embedding(domain, data_dir, flag_addition_corpus, flag_word2vec, flag_use_sentiment_embedding):\n",
    "    word_dict = dict()\n",
    "    embedding = list()\n",
    "\n",
    "#     if (flag_addition_corpus):\n",
    "#         # fix it if you want to add more data for word2vec training\n",
    "#         continue\n",
    "        \n",
    "    if (flag_word2vec):\n",
    "        os.system('cd ../fastText && ./fasttext cbow -input ../data/' + domain + '_corpus_for_word2vec.txt -output ../data/' + domain + '_cbow_final_2014 -dim 100 -minCount 0 -epoch 2000')\n",
    "    \n",
    "    sswe = defaultdict(list)\n",
    "    if (flag_use_sentiment_embedding):\n",
    "        f_se = codecs.open('../dictionary/sswe-u.txt', 'r', 'utf-8')\n",
    "        \n",
    "        for line in f_se:\n",
    "            elements = line.split()\n",
    "            for i in range(1, len(elements)):\n",
    "                sswe[elements[0].strip()].append(float(elements[i]))\n",
    "        f_se.close()\n",
    "\n",
    "    f_vec = codecs.open('../data/' + domain + '_cbow_final_2014.vec', 'r', 'utf-8')\n",
    "\n",
    "    idx = 0\n",
    "    for line in f_vec:\n",
    "        if len(line) < 50:\n",
    "            continue\n",
    "        else:\n",
    "            component = line.strip().split(' ')\n",
    "            word_dict[component[0].lower()] = idx\n",
    "            if (flag_use_sentiment_embedding and component[0].lower() in sswe.keys()):\n",
    "                embedding.append(sswe[component[0].lower()])\n",
    "            else:\n",
    "                word_vec = list()\n",
    "                for i in range(1, len(component)):\n",
    "                    word_vec.append(float(component[i]))\n",
    "                embedding.append(word_vec)\n",
    "            idx = idx + 1\n",
    "    f_vec.close()\n",
    "    word_dict['<padding>'] = idx\n",
    "    embedding.append([0.] * len(embedding[0]))\n",
    "    word_dict_rev = {v: k for k, v in word_dict.items()}\n",
    "    return word_dict, word_dict_rev, embedding\n",
    "\n",
    "\n",
    "def load_stop_words():\n",
    "    stop_words = list()\n",
    "    fsw = codecs.open('../dictionary/stop_words.txt', 'r', 'utf-8')\n",
    "    for line in fsw:\n",
    "        stop_words.append(line.strip())\n",
    "    fsw.close()\n",
    "    return stop_words\n",
    "\n",
    "\n",
    "def load_sentiment_dictionary():\n",
    "    pos_list = list()\n",
    "    neg_list = list()\n",
    "    rev_list = list()\n",
    "    inc_list = list()\n",
    "    dec_list = list()\n",
    "    sent_words_dict = dict()\n",
    "\n",
    "    fneg = open('../dictionary/negative_words.txt', 'r')\n",
    "    fpos = open('../dictionary/positive_words.txt', 'r')\n",
    "    frev = open('../dictionary/reverse_words.txt', 'r')\n",
    "    fdec = open('../dictionary/decremental_words.txt', 'r')\n",
    "    finc = open('../dictionary/incremental_words.txt', 'r')\n",
    "\n",
    "    for line in fpos:\n",
    "        if not line.strip() in sent_words_dict:\n",
    "            sent_words_dict[line.strip()] = 0\n",
    "            pos_list.append(line.strip())\n",
    "\n",
    "    for line in fneg:\n",
    "        if not line.strip() in sent_words_dict:\n",
    "            sent_words_dict[line.strip()] = 1\n",
    "            neg_list.append(line.strip())\n",
    "\n",
    "    for line in frev:\n",
    "        if not line.strip() in sent_words_dict:\n",
    "            sent_words_dict[line.strip()] = 2\n",
    "            rev_list.append(line.strip())\n",
    "\n",
    "    for line in finc:\n",
    "        if not line.strip() in sent_words_dict:\n",
    "            sent_words_dict[line.strip()] = 3\n",
    "            inc_list.append(line.strip())\n",
    "\n",
    "    for line in fdec:\n",
    "        if not line.strip() in sent_words_dict:\n",
    "            sent_words_dict[line.strip()] = 4\n",
    "            dec_list.append(line.strip())\n",
    "            \n",
    "    fneg.close()\n",
    "    fpos.close()\n",
    "    frev.close()\n",
    "    fdec.close()\n",
    "    finc.close()\n",
    "\n",
    "    return pos_list, neg_list, rev_list, inc_list, dec_list, sent_words_dict\n",
    "\n",
    "\n",
    "def export_aspect(domain, data_dir):\n",
    "    aspect_list = list()\n",
    "    \n",
    "    fa = codecs.open('../dictionary/' + domain + '_aspect.txt', 'w', 'utf-8')\n",
    "    for file in os.listdir(data_dir):\n",
    "        if not (file.endswith('.txt') and domain in file):\n",
    "            continue\n",
    "            \n",
    "        f = codecs.open(data_dir + file, 'r', 'utf-8')\n",
    "        for line in f:\n",
    "            for word in line.split(' '):\n",
    "                if '{as' in word:\n",
    "                    aspect_list.append(word.split('{')[0].strip())\n",
    "        f.close()\n",
    "            \n",
    "    for w in sorted(set(aspect_list)):\n",
    "        fa.write(w + '\\n')\n",
    "    \n",
    "    fa.close()\n",
    "    \n",
    "    return set(aspect_list)\n",
    "\n",
    "\n",
    "def sortchildrenby(parent, attr):\n",
    "    parent[:] = sorted(parent, key=lambda child: int(child.get(attr)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(domain, data_dir, flag_word2vec, label_dict, seq_max_len, flag_addition_corpus,\n",
    "            flag_change_xml_to_txt, negative_weight, positive_weight, neutral_weight, \n",
    "            flag_use_sentiment_embedding):\n",
    "\n",
    "    test_data = list()\n",
    "    test_mask = list()\n",
    "    test_binary_mask = list()\n",
    "#     test_label = list()\n",
    "    test_seq_len = list()\n",
    "    test_sentiment_for_word = list()\n",
    "\n",
    "    stop_words = load_stop_words()\n",
    "    pos_list, neg_list, rev_list, inc_list, dec_list, sent_words_dict = load_sentiment_dictionary()\n",
    "    aspect_list = export_aspect(domain, data_dir)\n",
    "    word_dict, word_dict_rev, embedding = load_embedding(domain, data_dir, flag_addition_corpus, flag_word2vec, flag_use_sentiment_embedding)\n",
    "    # load data, mask, label\n",
    "    for file in os.listdir(data_dir):\n",
    "        if not (file.endswith('.txt') and domain in file):\n",
    "            continue\n",
    "            \n",
    "        f_processed = codecs.open(data_dir + file, 'r', 'utf-8')\n",
    "        for line in f_processed:\n",
    "            data_tmp = list()\n",
    "            mask_tmp = list()\n",
    "            binary_mask_tmp = list()\n",
    "            label_tmp = list()\n",
    "            sentiment_for_word_tmp = list()\n",
    "            count_len = 0\n",
    "\n",
    "            words = line.strip().split(' ')\n",
    "            for word in words:\n",
    "                if (word in stop_words):\n",
    "                    continue\n",
    "                word_clean = word.replace('{aspositive}', '').replace('{asnegative}', '').replace('{asneutral}', '')\n",
    "\n",
    "                if (word_clean in word_dict.keys() and count_len < seq_max_len):\n",
    "                    if (word_clean in pos_list):\n",
    "                        sentiment_for_word_tmp.append(1)\n",
    "                    elif (word_clean in neg_list):\n",
    "                        sentiment_for_word_tmp.append(2)\n",
    "                    elif (word_clean in rev_list):\n",
    "                        sentiment_for_word_tmp.append(0)\n",
    "                    elif (word_clean in inc_list):\n",
    "                        sentiment_for_word_tmp.append(0)\n",
    "                    elif (word_clean in dec_list):\n",
    "                        sentiment_for_word_tmp.append(0)\n",
    "                    else:\n",
    "                        sentiment_for_word_tmp.append(0)\n",
    "\n",
    "                    if ('aspositive' in word):\n",
    "                        mask_tmp.append(positive_weight)\n",
    "                        binary_mask_tmp.append(1.0)\n",
    "                        label_tmp.append(label_dict['aspositive'])\n",
    "                        count_pos = count_pos + 1\n",
    "                    elif ('asneutral' in word):\n",
    "                        mask_tmp.append(neutral_weight)\n",
    "                        binary_mask_tmp.append(1.0)\n",
    "                        label_tmp.append(label_dict['asneutral'])\n",
    "                        count_neu = count_neu + 1\n",
    "                    elif ('asnegative' in word):\n",
    "                        mask_tmp.append(negative_weight)\n",
    "                        binary_mask_tmp.append(1.0)\n",
    "                        label_tmp.append(label_dict['asnegative'])\n",
    "                        count_neg = count_neg + 1\n",
    "                    else:\n",
    "                        mask_tmp.append(0.)\n",
    "                        binary_mask_tmp.append(0.)\n",
    "                        label_tmp.append(0)\n",
    "                    count_len = count_len + 1\n",
    "\n",
    "                    data_tmp.append(word_dict[word_clean])\n",
    "#                 elif '{as' in word and file != domain + '_Train_Final.txt':\n",
    "#                     print(word)\n",
    "\n",
    "            if file == domain + '_Train_Final.txt':\n",
    "                train_seq_len.append(count_len)\n",
    "            else:\n",
    "                test_seq_len.append(count_len)\n",
    "\n",
    "            for _ in range(seq_max_len - count_len):\n",
    "                data_tmp.append(word_dict['<padding>'])\n",
    "                mask_tmp.append(0.)\n",
    "                binary_mask_tmp.append(0.)\n",
    "                label_tmp.append(0)\n",
    "                sentiment_for_word_tmp.append(0)\n",
    "\n",
    "            if file == domain + '_Train_Final.txt':\n",
    "                train_data.append(data_tmp)\n",
    "                train_mask.append(mask_tmp)\n",
    "                train_binary_mask.append(binary_mask_tmp)\n",
    "                train_label.append(label_tmp)\n",
    "                train_sentiment_for_word.append(sentiment_for_word_tmp)\n",
    "            else:\n",
    "                test_data.append(data_tmp)\n",
    "                test_mask.append(m ask_tmp)\n",
    "                test_binary_mask.append(binary_mask_tmp)\n",
    "                test_label.append(label_tmp)\n",
    "                test_sentiment_for_word.append(sentiment_for_word_tmp)\n",
    "        f_processed.close()\n",
    "\n",
    "    print('pos: %d' %count_pos)\n",
    "    print('neu: %d' %count_neu)\n",
    "    print('neg: %d' %count_neg)\n",
    "    print('len of train data is %d' %(len(train_data)))\n",
    "    print('len of test data is %d' %(len(test_data)))\n",
    "#     data_sample = ''\n",
    "#     for id in train_data[10]:\n",
    "#         data_sample = data_sample + ' ' + word_dict_rev[id]\n",
    "\n",
    "#     print('%s' %data_sample)\n",
    "#     print(train_data[10])\n",
    "#     print(train_mask[10])\n",
    "#     print(train_label[10])\n",
    "#     print(train_sentiment_for_word[10])\n",
    "#     print('len of word dictionary is %d' %(len(word_dict)))\n",
    "#     print('len of embedding is %d' %(len(embedding)))\n",
    "#     print('len of aspect_list is %d' %(len(aspect_list)))\n",
    "#     print('max sequence length is %d' %(np.max(test_seq_len)))\n",
    "\n",
    "    return train_data, train_mask, train_binary_mask, train_label, train_seq_len, train_sentiment_for_word, test_data, test_mask, test_binary_mask, test_label, test_seq_len, test_sentiment_for_word, word_dict, word_dict_rev, embedding, aspect_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '../data/ABSA_SemEval2014/'\n",
    "domain = 'Laptops'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = list()\n",
    "test_mask = list()\n",
    "test_binary_mask = list()\n",
    "# test_label = list()\n",
    "test_seq_len = list()\n",
    "test_sentiment_for_word = list()\n",
    "\n",
    "stop_words = load_stop_words()\n",
    "pos_list, neg_list, rev_list, inc_list, dec_list, sent_words_dict = load_sentiment_dictionary()\n",
    "# aspect_list = export_aspect(domain, data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
