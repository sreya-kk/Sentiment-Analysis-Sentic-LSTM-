{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import codecs\n",
    "import math\n",
    "import numpy as np\n",
    "import utils\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from pylab import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining required function and objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Objects to load training and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Data:\n",
    "    def __init__(self, domain, data_dir, flag_word2vec, label_dict, seq_max_len, flag_addition_corpus,\n",
    "     flag_change_file_structure, negative_weight, positive_weight, neutral_weight, flag_use_sentiment_embedding):\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.label_dict = label_dict\n",
    "        self.domain = domain\n",
    "        self.data_dir = data_dir\n",
    "        self.flag_word2vec = flag_word2vec\n",
    "        self.flag_addition_corpus = flag_addition_corpus\n",
    "        self.flag_change_file_structure = flag_change_file_structure\n",
    "        self.negative_weight = negative_weight\n",
    "        self.positive_weight = positive_weight\n",
    "        self.neutral_weight = neutral_weight\n",
    "        self.flag_use_sentiment_embedding = flag_use_sentiment_embedding\n",
    "\n",
    "        self.train_data, self.train_mask, self.train_binary_mask, self.train_label, self.train_seq_len, self.train_sentiment_for_word, \\\n",
    "        self.test_data, self.test_mask, self.test_binary_mask, self.test_label, self.test_seq_len, self.test_sentiment_for_word, \\\n",
    "        self.word_dict, self.word_dict_rev, self.embedding, aspect_list = utils.load_data(\n",
    "            self.domain,\n",
    "            self.data_dir,\n",
    "            self.flag_word2vec,\n",
    "            self.label_dict,\n",
    "            self.seq_max_len,\n",
    "            self.flag_addition_corpus,\n",
    "            self.flag_change_file_structure,\n",
    "            self.negative_weight,\n",
    "            self.positive_weight,\n",
    "            self.neutral_weight,\n",
    "            self.flag_use_sentiment_embedding\n",
    "        )\n",
    "\n",
    "        self.nb_sample_train = len(self.train_data)\n",
    "        \n",
    "        self.x_test = list()\n",
    "        for i in range(len(self.test_data)):\n",
    "            sentence = list()\n",
    "            for word_id in self.test_data[i]:\n",
    "                sentence.append(self.embedding[word_id])\n",
    "            self.x_test.append(sentence)\n",
    "            \n",
    "        self.x_train = list()\n",
    "        for i in range(len(self.train_data)):\n",
    "            sentence = list()\n",
    "            for word_id in self.train_data[i]:\n",
    "                sentence.append(self.embedding[word_id])\n",
    "            self.x_train.append(sentence)\n",
    "\n",
    "    def parse_data(self, sentences):\n",
    "        self.test_data = list()\n",
    "        self.test_binary_mask = list()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            words = sentence.split(' ')\n",
    "            data_tmp = list()\n",
    "            binary_mask_tmp = list()\n",
    "            count_len = 0\n",
    "            for word in words:\n",
    "                if (word in word_dict.keys() and count_len < seq_max_len):\n",
    "                    data_tmp.append(self.word_dict[word])\n",
    "                    binary_mask_tmp.append(1.0)\n",
    "                    count_len = count_len + 1\n",
    "    \n",
    "            for _ in range(self.seq_max_len - count_len):\n",
    "                data_tmp.append(word_dict['<padding>'])\n",
    "                binary_mask_tmp.append(0.)\n",
    "\n",
    "            self.test_data.append(data_tmp)\n",
    "            self.test_binary_mask.append(binary_mask_tmp)\n",
    "\n",
    "        self.x_test = list()\n",
    "        for i in range(len(self.test_data)):\n",
    "            sentence = list()\n",
    "            for word_id in self.test_data[i]:\n",
    "                sentence.append(self.embedding[word_id])\n",
    "            self.x_test.append(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Object to create model and defining funtions for training and evaluating"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model:\n",
    "    def __init__(self, batch_size, seq_max_len, nb_sentiment_label, nb_sentiment_for_word, embedding_size, nb_linear_inside,\n",
    "        nb_lstm_inside, layers, TRAINING_ITERATIONS, LEARNING_RATE, WEIGHT_DECAY, flag_train, flag_use_sentiment_for_word, sess):\n",
    "        self.batch_size = batch_size\n",
    "        self.seq_max_len = seq_max_len\n",
    "        self.nb_sentiment_label = nb_sentiment_label\n",
    "        self.nb_sentiment_for_word = nb_sentiment_for_word\n",
    "        self.embedding_size = embedding_size\n",
    "        self.nb_linear_inside = nb_linear_inside\n",
    "        self.nb_lstm_inside = nb_lstm_inside\n",
    "        self.layers = layers\n",
    "        self.TRAINING_ITERATIONS = TRAINING_ITERATIONS\n",
    "        self.LEARNING_RATE = LEARNING_RATE\n",
    "        self.WEIGHT_DECAY = WEIGHT_DECAY\n",
    "        self.flag_train = flag_train\n",
    "        self.flag_use_sentiment_for_word = flag_use_sentiment_for_word\n",
    "        self.sess = sess\n",
    "\n",
    "    def modeling(self):\n",
    "        self.tf_X_train = tf.placeholder(tf.float32, \\\n",
    "            shape = [None, self.seq_max_len, self.embedding_size - int(self.flag_use_sentiment_for_word) * self.nb_sentiment_for_word])\n",
    "        self.tf_X_sent_for_word = tf.placeholder(tf.int64, shape = [None, self.seq_max_len])\n",
    "        self.tf_X_train_mask = tf.placeholder(tf.float32, shape = [None, self.seq_max_len])\n",
    "        self.tf_X_binary_mask = tf.placeholder(tf.float32, shape = [None, self.seq_max_len])\n",
    "        self.tf_y_train = tf.placeholder(tf.int64, shape = [None, self.seq_max_len])\n",
    "        self.tf_X_seq_len = tf.placeholder(tf.int64, shape = [None])\n",
    "        self.keep_prob = tf.placeholder(tf.float32)\n",
    "        \n",
    "\n",
    "        self.ln_w = tf.Variable(tf.truncated_normal([self.embedding_size, self.nb_linear_inside], stddev = math.sqrt(3.0 / (self.embedding_size + self.nb_linear_inside))))\n",
    "        self.ln_b = tf.Variable(tf.zeros([self.nb_linear_inside]))\n",
    "         \n",
    "        self.sent_w = tf.Variable(tf.truncated_normal([self.nb_lstm_inside, self.nb_sentiment_label],\n",
    "                                                 stddev = math.sqrt(3.0 / self.nb_lstm_inside + self.nb_sentiment_label)))\n",
    "        self.sent_b = tf.Variable(tf.zeros([self.nb_sentiment_label]))\n",
    "\n",
    "        y_labels = tf.one_hot(self.tf_y_train,\n",
    "                              self.nb_sentiment_label,\n",
    "                              on_value = 1.0,\n",
    "                              off_value = 0.0,\n",
    "                              axis = -1)\n",
    "         \n",
    "\n",
    "        if (self.flag_use_sentiment_for_word):\n",
    "            X_sent_for_word = tf.one_hot(self.tf_X_sent_for_word, self.nb_sentiment_for_word,\n",
    "                                 on_value = 20.0,\n",
    "                                 off_value = 10.0,\n",
    "                                 axis = -1)\n",
    "\n",
    "            X_train = tf.concat([self.tf_X_train, X_sent_for_word], 2)\n",
    "            X_train = tf.transpose(X_train, [1, 0, 2])\n",
    "        else:\n",
    "            X_train = tf.transpose(self.tf_X_train, [1, 0, 2])\n",
    "\n",
    "        # Reshaping to (n_steps * batch_size, n_input)\n",
    "        X_train = tf.reshape(X_train, [-1, self.embedding_size])\n",
    "        X_train = tf.nn.relu(tf.add(tf.matmul(X_train, self.ln_w), self.ln_b))\n",
    "        X_train = tf.nn.dropout(X_train, self.keep_prob)\n",
    "        X_train = tf.split(axis = 0, num_or_size_splits = self.seq_max_len, value = X_train)\n",
    "        \n",
    "        # bidirection lstm\n",
    "        # Creating the forward and backwards cells\n",
    "        lstm_fw_cell = tf.nn.rnn_cell.BasicLSTMCell(self.nb_lstm_inside, forget_bias = 0.8)\n",
    "        lstm_bw_cell = tf.nn.rnn_cell.BasicLSTMCell(self.nb_lstm_inside, forget_bias = 0.8)\n",
    "        # Get lstm cell output\n",
    "        outputs, _, _ = tf.contrib.rnn.static_bidirectional_rnn(lstm_fw_cell,\n",
    "                                                     lstm_bw_cell,\n",
    "                                                     X_train,\n",
    "                                                     dtype='float32')\n",
    "\n",
    "        output_fw, output_bw = tf.split(outputs, [self.nb_lstm_inside, self.nb_lstm_inside], 2)\n",
    "        sentiment = tf.reshape(tf.add(output_fw, output_bw), [-1, self.nb_lstm_inside])\n",
    "        \n",
    "        # sentiment = tf.reshape(outputs, [-1, 2 * self.nb_lstm_inside]) \n",
    "        sentiment = tf.nn.dropout(sentiment, self.keep_prob)\n",
    "        sentiment = tf.add(tf.matmul(sentiment, self.sent_w), self.sent_b)\n",
    "        sentiment = tf.split(axis = 0, num_or_size_splits = self.seq_max_len, value = sentiment)\n",
    "\n",
    "        # Change back dimension to [batch_size, n_step, n_input]\n",
    "        sentiment = tf.stack(sentiment)\n",
    "        sentiment = tf.transpose(sentiment, [1, 0, 2])\n",
    "        sentiment = tf.multiply(sentiment, tf.expand_dims(self.tf_X_binary_mask, 2))\n",
    "\n",
    "        self.cross_entropy = tf.reduce_mean(tf.multiply(tf.nn.softmax_cross_entropy_with_logits(logits = sentiment, labels = y_labels), self.tf_X_train_mask))\n",
    "        \n",
    "        regularization = self.WEIGHT_DECAY * sum(\n",
    "            tf.nn.l2_loss(tf_var)\n",
    "                for tf_var in tf.trainable_variables()\n",
    "                if not (\"noreg\" in tf_var.name or \"Bias\" in tf_var.name)\n",
    "        )\n",
    "        self.cross_entropy = self.cross_entropy + regularization\n",
    "        \n",
    "        self.prediction = tf.argmax(tf.nn.softmax(sentiment), 2)\n",
    "        self.correct_prediction = tf.reduce_sum(tf.multiply(tf.cast(tf.equal(self.prediction, self.tf_y_train), tf.float32), self.tf_X_binary_mask))\n",
    "        self.global_step = tf.Variable(0, trainable = True)\n",
    "        self.learning_rate = tf.train.exponential_decay(self.LEARNING_RATE, self.global_step, 1000, 0.65, staircase = True)\n",
    "        # self.optimizer = tf.train.GradientDescentOptimizer(self.learning_rate).minimize(self.cross_entropy, global_step = self.global_step)\n",
    "        self.optimizer = tf.train.AdagradOptimizer(self.learning_rate).minimize(self.cross_entropy)\n",
    "\n",
    "        self.saver = tf.train.Saver()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "\n",
    "\n",
    "    def load_model(self):\n",
    "        self.saver.restore(self.sess, '../ckpt/se-apect-v0.ckpt')\n",
    "\n",
    "    def save_model(self):\n",
    "        self.saver.save(self.sess, '../ckpt/se-apect-v0.ckpt')\n",
    "\n",
    "    def predict(self, data):\n",
    "        prediction_test = self.sess.run(self.prediction, \n",
    "                          feed_dict={self.tf_X_train: np.asarray(data.x_test),\n",
    "                                     self.tf_X_binary_mask: np.asarray(data.test_binary_mask),\n",
    "                                     self.tf_X_seq_len: np.asarray(data.test_seq_len),\n",
    "                                     self.tf_X_sent_for_word: np.asarray(data.test_sentiment_for_word),\n",
    "                                     self.keep_prob: 1.0})\n",
    "\n",
    "\n",
    "        ret = list()\n",
    "        for i in range(len(data.test_data)):\n",
    "                data_sample = ''\n",
    "                for j in range(len(data.test_data[i])):\n",
    "                    if data.word_dict_rev[data.test_data[i][j]] == '<unk>':\n",
    "                        continue\n",
    "                    elif data.test_binary_mask[i][j] > 0.:\n",
    "                        data_sample = data_sample + data.word_dict_rev[data.test_data[i][j]] + \\\n",
    "                         '(predict ' + str(prediction_test[i][j]) + ') '\n",
    "                    else:\n",
    "                        data_sample = data_sample + data.word_dict_rev[data.test_data[i][j]] + ' '\n",
    "                ret.append(data_sample.replace('<padding>', '').strip())\n",
    "        return ret\n",
    "\n",
    "\n",
    "    def evaluate(self, data, flag_write_to_file, flag_train):\n",
    "        if (not flag_train):\n",
    "            self.load_model()\n",
    "\n",
    "        correct_prediction_test, prediction_test = self.sess.run([self.correct_prediction, self.prediction], \n",
    "                                              feed_dict={self.tf_X_train: np.asarray(data.x_test),\n",
    "                                                         self.tf_X_binary_mask: np.asarray(data.test_binary_mask),\n",
    "                                                         self.tf_X_seq_len: np.asarray(data.test_seq_len),\n",
    "                                                         self.tf_X_sent_for_word: np.asarray(data.test_sentiment_for_word),\n",
    "                                                         self.tf_y_train: np.asarray(data.test_label),\n",
    "                                                         self.keep_prob: 1.0})\n",
    "\n",
    "        print('test accuracy => %.3f' %(float(correct_prediction_test)/np.sum(data.test_binary_mask)))\n",
    "\n",
    "        if float(correct_prediction_test)/np.sum(data.test_binary_mask) > 0.809:\n",
    "            self.save_model()\n",
    "\n",
    "        if (flag_write_to_file):\n",
    "            f_result = codecs.open('../result/result.txt', 'w', 'utf-8')\n",
    "            f_result.write('#---------------------------------------------------------------------------------------------------------#\\n')\n",
    "            f_result.write('#\\t author: BinhDT\\n')\n",
    "            f_result.write('#\\t test accuracy %.2f\\n' %(float(correct_prediction_test)*100/np.sum(np.asarray(data.test_binary_mask) > 0.)))\n",
    "            f_result.write('#\\t 1:positive, 0:neutral, 2:negative\\n')\n",
    "            f_result.write('#---------------------------------------------------------------------------------------------------------#\\n')\n",
    "\n",
    "            for i in range(len(data.test_data)):\n",
    "                data_sample = ''\n",
    "                for j in range(len(data.test_data[i])):\n",
    "                    if data.word_dict_rev[data.test_data[i][j]] == '<unk>':\n",
    "                        continue\n",
    "                    elif data.test_binary_mask[i][j] > 0.:\n",
    "                        data_sample = data_sample + data.word_dict_rev[data.test_data[i][j]] + '(label ' + str(data.test_label[i][j]) + \\\n",
    "                         '|predict ' + str(prediction_test[i][j]) + ') '\n",
    "                    else:\n",
    "                        data_sample = data_sample + data.word_dict_rev[data.test_data[i][j]] + ' '\n",
    "                f_result.write('%s\\n' %data_sample.replace('<padding>', '').strip())\n",
    "\n",
    "            f_result.close()\n",
    "\n",
    "    def train(self, data):\n",
    "        self.sess.run(self.init)\n",
    "        # self.load_model()\n",
    "        loss_list = list()\n",
    "        accuracy_list = list()\n",
    "\n",
    "        for it in range(self.TRAINING_ITERATIONS):\n",
    "            # generate batch (x_train, y_train, seq_lengths_train)\n",
    "            if (it * self.batch_size % data.nb_sample_train + self.batch_size < data.nb_sample_train):\n",
    "                index = it * self.batch_size % data.nb_sample_train\n",
    "            else:\n",
    "                index = data.nb_sample_train - self.batch_size\n",
    "            \n",
    "\n",
    "            self.sess.run(self.optimizer, \n",
    "                          feed_dict={self.tf_X_train: np.asarray(data.x_train[index : index + self.batch_size]),\n",
    "                                     self.tf_X_train_mask: np.asarray(data.train_mask[index : index + self.batch_size]),\n",
    "                                     self.tf_X_binary_mask: np.asarray(data.train_binary_mask[index : index + self.batch_size]),\n",
    "                                     self.tf_X_seq_len: np.asarray(data.train_seq_len[index : index + self.batch_size]),\n",
    "                                     self.tf_X_sent_for_word: np.asarray(data.train_sentiment_for_word[index : index + self.batch_size]),\n",
    "                                     self.tf_y_train: np.asarray(data.train_label[index : index + self.batch_size]),\n",
    "                                     self.keep_prob: 0.5})\n",
    "\n",
    "            if it % (len(data.x_train) // self.batch_size) == 0:\n",
    "                print(it)\n",
    "                self.evaluate(data, it + 100 >= self.TRAINING_ITERATIONS, self.flag_train)\n",
    "                '''\n",
    "                correct_prediction_train, cost_train = self.sess.run([self.correct_prediction, self.cross_entropy], \n",
    "                                                  feed_dict={self.tf_X_train: np.asarray(data.x_train),\n",
    "                                                             self.tf_X_train_mask: np.asarray(data.train_mask),\n",
    "                                                             self.tf_X_binary_mask: np.asarray(data.train_binary_mask),\n",
    "                                                             self.tf_X_seq_len: np.asarray(data.train_seq_len),\n",
    "                                                             self.tf_X_sent_for_word: np.asarray(data.train_sentiment_for_word),\n",
    "                                                             self.tf_y_train: np.asarray(data.train_label),\n",
    "                                                             self.keep_prob: 0.8})\n",
    "                \n",
    "                print('training_accuracy => %.3f, cost value => %.5f for step %d' % \\\n",
    "                (float(correct_prediction_train)/np.sum(np.asarray(data.train_binary_mask)), cost_train, it))\n",
    "                \n",
    "                loss_list.append(cost_train)\n",
    "                accuracy_list.append(float(correct_prediction_train)/np.sum(np.asarray(data.train_binary_mask)))\n",
    "                \n",
    "                _, ax1 = plt.subplots()\n",
    "                ax2 = ax1.twinx()\n",
    "                ax1.plot(loss_list)\n",
    "                ax2.plot(accuracy_list, 'r')\n",
    "                ax1.set_xlabel('epoch')\n",
    "                ax1.set_ylabel('train loss')\n",
    "                ax2.set_ylabel('train accuracy')\n",
    "                ax1.set_title('train accuracy and loss')\n",
    "                ax1.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "                plt.savefig('accuracy_loss.eps', format='eps', dpi=150)\n",
    "                plt.close()\n",
    "                '''\n",
    "\n",
    "\n",
    "#         self.sess.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "nb_sentiment_label = 3\n",
    "nb_sentiment_for_word = 3\n",
    "embedding_size = 100\n",
    "nb_lstm_inside = 256\n",
    "layers = 1\n",
    "TRAINING_ITERATIONS = 5000\n",
    "LEARNING_RATE = 0.1\n",
    "WEIGHT_DECAY = 0.0001\n",
    "label_dict = {\n",
    "    'aspositive' : 1,\n",
    "    'asneutral' : 0,\n",
    "    'asnegative': 2\n",
    "}\n",
    "data_dir = '../data/ABSA_SemEval2014/'\n",
    "domain = 'Restaurants'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required for resturant 2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_max_len = 35\n",
    "nb_linear_inside = 512"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "flag_word2vec = False\n",
    "flag_addition_corpus = False\n",
    "flag_change_file_structure = False\n",
    "flag_use_sentiment_embedding = False\n",
    "flag_use_sentiment_for_word = True\n",
    "flag_train = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_weight = 1.0\n",
    "positive_weight = 1.0\n",
    "neutral_weight = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos: 1431\n",
      "neu: 474\n",
      "neg: 539\n",
      "len of train data is 2021\n",
      "len of test data is 606\n"
     ]
    }
   ],
   "source": [
    "data = Data(domain, data_dir, flag_word2vec, label_dict, seq_max_len, flag_addition_corpus, flag_change_file_structure, negative_weight, positive_weight, neutral_weight, flag_use_sentiment_embedding)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-78874aa4d837>:58: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From <ipython-input-4-78874aa4d837>:63: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From <ipython-input-4-78874aa4d837>:69: static_bidirectional_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.Bidirectional(keras.layers.RNN(cell, unroll=True))`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\ops\\rnn.py:1610: static_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell, unroll=True)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\ops\\rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From <ipython-input-4-78874aa4d837>:84: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\aritra banerjee\\envs\\dl\\lib\\site-packages\\ipykernel_launcher.py:88: DeprecationWarning: Calling np.sum(generator) is deprecated, and in the future will give a different result. Use np.sum(np.fromiter(generator)) or the python sum builtin instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\aritra banerjee\\envs\\dl\\lib\\site-packages\\tensorflow\\python\\training\\adagrad.py:76: calling Constant.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "0\n",
      "test accuracy => 0.657\n",
      "15\n",
      "test accuracy => 0.149\n",
      "30\n",
      "test accuracy => 0.657\n",
      "45\n",
      "test accuracy => 0.304\n",
      "60\n",
      "test accuracy => 0.657\n",
      "75\n",
      "test accuracy => 0.657\n",
      "90\n",
      "test accuracy => 0.657\n",
      "105\n",
      "test accuracy => 0.623\n",
      "120\n",
      "test accuracy => 0.657\n",
      "135\n",
      "test accuracy => 0.657\n",
      "150\n",
      "test accuracy => 0.657\n",
      "165\n",
      "test accuracy => 0.657\n",
      "180\n",
      "test accuracy => 0.657\n",
      "195\n",
      "test accuracy => 0.657\n",
      "210\n",
      "test accuracy => 0.657\n",
      "225\n",
      "test accuracy => 0.657\n",
      "240\n",
      "test accuracy => 0.657\n",
      "255\n",
      "test accuracy => 0.657\n",
      "270\n",
      "test accuracy => 0.657\n",
      "285\n",
      "test accuracy => 0.657\n",
      "300\n",
      "test accuracy => 0.657\n",
      "315\n",
      "test accuracy => 0.657\n",
      "330\n",
      "test accuracy => 0.657\n",
      "345\n",
      "test accuracy => 0.657\n",
      "360\n",
      "test accuracy => 0.657\n",
      "375\n",
      "test accuracy => 0.657\n",
      "390\n",
      "test accuracy => 0.657\n",
      "405\n",
      "test accuracy => 0.657\n",
      "420\n",
      "test accuracy => 0.657\n",
      "435\n",
      "test accuracy => 0.657\n",
      "450\n",
      "test accuracy => 0.657\n",
      "465\n",
      "test accuracy => 0.657\n",
      "480\n",
      "test accuracy => 0.657\n",
      "495\n",
      "test accuracy => 0.657\n",
      "510\n",
      "test accuracy => 0.657\n",
      "525\n",
      "test accuracy => 0.657\n",
      "540\n",
      "test accuracy => 0.657\n",
      "555\n",
      "test accuracy => 0.657\n",
      "570\n",
      "test accuracy => 0.657\n",
      "585\n",
      "test accuracy => 0.657\n",
      "600\n",
      "test accuracy => 0.657\n",
      "615\n",
      "test accuracy => 0.657\n",
      "630\n",
      "test accuracy => 0.657\n",
      "645\n",
      "test accuracy => 0.657\n",
      "660\n",
      "test accuracy => 0.657\n",
      "675\n",
      "test accuracy => 0.657\n",
      "690\n",
      "test accuracy => 0.659\n",
      "705\n",
      "test accuracy => 0.668\n",
      "720\n",
      "test accuracy => 0.657\n",
      "735\n",
      "test accuracy => 0.659\n",
      "750\n",
      "test accuracy => 0.657\n",
      "765\n",
      "test accuracy => 0.659\n",
      "780\n",
      "test accuracy => 0.659\n",
      "795\n",
      "test accuracy => 0.657\n",
      "810\n",
      "test accuracy => 0.660\n",
      "825\n",
      "test accuracy => 0.657\n",
      "840\n",
      "test accuracy => 0.657\n",
      "855\n",
      "test accuracy => 0.659\n",
      "870\n",
      "test accuracy => 0.657\n",
      "885\n",
      "test accuracy => 0.657\n",
      "900\n",
      "test accuracy => 0.659\n",
      "915\n",
      "test accuracy => 0.681\n",
      "930\n",
      "test accuracy => 0.666\n",
      "945\n",
      "test accuracy => 0.666\n",
      "960\n",
      "test accuracy => 0.662\n",
      "975\n",
      "test accuracy => 0.683\n",
      "990\n",
      "test accuracy => 0.690\n",
      "1005\n",
      "test accuracy => 0.681\n",
      "1020\n",
      "test accuracy => 0.679\n",
      "1035\n",
      "test accuracy => 0.690\n",
      "1050\n",
      "test accuracy => 0.675\n",
      "1065\n",
      "test accuracy => 0.666\n",
      "1080\n",
      "test accuracy => 0.679\n",
      "1095\n",
      "test accuracy => 0.688\n",
      "1110\n",
      "test accuracy => 0.681\n",
      "1125\n",
      "test accuracy => 0.688\n",
      "1140\n",
      "test accuracy => 0.681\n",
      "1155\n",
      "test accuracy => 0.674\n",
      "1170\n",
      "test accuracy => 0.683\n",
      "1185\n",
      "test accuracy => 0.679\n",
      "1200\n",
      "test accuracy => 0.687\n",
      "1215\n",
      "test accuracy => 0.687\n",
      "1230\n",
      "test accuracy => 0.670\n",
      "1245\n",
      "test accuracy => 0.675\n",
      "1260\n",
      "test accuracy => 0.683\n",
      "1275\n",
      "test accuracy => 0.692\n",
      "1290\n",
      "test accuracy => 0.698\n",
      "1305\n",
      "test accuracy => 0.703\n",
      "1320\n",
      "test accuracy => 0.688\n",
      "1335\n",
      "test accuracy => 0.681\n",
      "1350\n",
      "test accuracy => 0.685\n",
      "1365\n",
      "test accuracy => 0.683\n",
      "1380\n",
      "test accuracy => 0.690\n",
      "1395\n",
      "test accuracy => 0.683\n",
      "1410\n",
      "test accuracy => 0.685\n",
      "1425\n",
      "test accuracy => 0.688\n",
      "1440\n",
      "test accuracy => 0.688\n",
      "1455\n",
      "test accuracy => 0.688\n",
      "1470\n",
      "test accuracy => 0.698\n",
      "1485\n",
      "test accuracy => 0.690\n",
      "1500\n",
      "test accuracy => 0.687\n",
      "1515\n",
      "test accuracy => 0.687\n",
      "1530\n",
      "test accuracy => 0.672\n",
      "1545\n",
      "test accuracy => 0.698\n",
      "1560\n",
      "test accuracy => 0.687\n",
      "1575\n",
      "test accuracy => 0.698\n",
      "1590\n",
      "test accuracy => 0.674\n",
      "1605\n",
      "test accuracy => 0.700\n",
      "1620\n",
      "test accuracy => 0.677\n",
      "1635\n",
      "test accuracy => 0.701\n",
      "1650\n",
      "test accuracy => 0.696\n",
      "1665\n",
      "test accuracy => 0.696\n",
      "1680\n",
      "test accuracy => 0.687\n",
      "1695\n",
      "test accuracy => 0.696\n",
      "1710\n",
      "test accuracy => 0.696\n",
      "1725\n",
      "test accuracy => 0.692\n",
      "1740\n",
      "test accuracy => 0.690\n",
      "1755\n",
      "test accuracy => 0.711\n",
      "1770\n",
      "test accuracy => 0.700\n",
      "1785\n",
      "test accuracy => 0.690\n",
      "1800\n",
      "test accuracy => 0.700\n",
      "1815\n",
      "test accuracy => 0.698\n",
      "1830\n",
      "test accuracy => 0.713\n",
      "1845\n",
      "test accuracy => 0.698\n",
      "1860\n",
      "test accuracy => 0.694\n",
      "1875\n",
      "test accuracy => 0.705\n",
      "1890\n",
      "test accuracy => 0.707\n",
      "1905\n",
      "test accuracy => 0.694\n",
      "1920\n",
      "test accuracy => 0.700\n",
      "1935\n",
      "test accuracy => 0.703\n",
      "1950\n",
      "test accuracy => 0.701\n",
      "1965\n",
      "test accuracy => 0.690\n",
      "1980\n",
      "test accuracy => 0.696\n",
      "1995\n",
      "test accuracy => 0.694\n",
      "2010\n",
      "test accuracy => 0.694\n",
      "2025\n",
      "test accuracy => 0.711\n",
      "2040\n",
      "test accuracy => 0.705\n",
      "2055\n",
      "test accuracy => 0.711\n",
      "2070\n",
      "test accuracy => 0.713\n",
      "2085\n",
      "test accuracy => 0.713\n",
      "2100\n",
      "test accuracy => 0.715\n",
      "2115\n",
      "test accuracy => 0.713\n",
      "2130\n",
      "test accuracy => 0.698\n",
      "2145\n",
      "test accuracy => 0.715\n",
      "2160\n",
      "test accuracy => 0.701\n",
      "2175\n",
      "test accuracy => 0.703\n",
      "2190\n",
      "test accuracy => 0.709\n",
      "2205\n",
      "test accuracy => 0.692\n",
      "2220\n",
      "test accuracy => 0.709\n",
      "2235\n",
      "test accuracy => 0.715\n",
      "2250\n",
      "test accuracy => 0.711\n",
      "2265\n",
      "test accuracy => 0.713\n",
      "2280\n",
      "test accuracy => 0.718\n",
      "2295\n",
      "test accuracy => 0.716\n",
      "2310\n",
      "test accuracy => 0.713\n",
      "2325\n",
      "test accuracy => 0.716\n",
      "2340\n",
      "test accuracy => 0.713\n",
      "2355\n",
      "test accuracy => 0.715\n",
      "2370\n",
      "test accuracy => 0.718\n",
      "2385\n",
      "test accuracy => 0.715\n",
      "2400\n",
      "test accuracy => 0.722\n",
      "2415\n",
      "test accuracy => 0.718\n",
      "2430\n",
      "test accuracy => 0.707\n",
      "2445\n",
      "test accuracy => 0.694\n",
      "2460\n",
      "test accuracy => 0.707\n",
      "2475\n",
      "test accuracy => 0.718\n",
      "2490\n",
      "test accuracy => 0.709\n",
      "2505\n",
      "test accuracy => 0.724\n",
      "2520\n",
      "test accuracy => 0.722\n",
      "2535\n",
      "test accuracy => 0.718\n",
      "2550\n",
      "test accuracy => 0.715\n",
      "2565\n",
      "test accuracy => 0.713\n",
      "2580\n",
      "test accuracy => 0.711\n",
      "2595\n",
      "test accuracy => 0.718\n",
      "2610\n",
      "test accuracy => 0.724\n",
      "2625\n",
      "test accuracy => 0.724\n",
      "2640\n",
      "test accuracy => 0.728\n",
      "2655\n",
      "test accuracy => 0.722\n",
      "2670\n",
      "test accuracy => 0.726\n",
      "2685\n",
      "test accuracy => 0.724\n",
      "2700\n",
      "test accuracy => 0.722\n",
      "2715\n",
      "test accuracy => 0.722\n",
      "2730\n",
      "test accuracy => 0.701\n",
      "2745\n",
      "test accuracy => 0.705\n",
      "2760\n",
      "test accuracy => 0.720\n",
      "2775\n",
      "test accuracy => 0.713\n",
      "2790\n",
      "test accuracy => 0.724\n",
      "2805\n",
      "test accuracy => 0.707\n",
      "2820\n",
      "test accuracy => 0.718\n",
      "2835\n",
      "test accuracy => 0.722\n",
      "2850\n",
      "test accuracy => 0.716\n",
      "2865\n",
      "test accuracy => 0.715\n",
      "2880\n",
      "test accuracy => 0.713\n",
      "2895\n",
      "test accuracy => 0.726\n",
      "2910\n",
      "test accuracy => 0.729\n",
      "2925\n",
      "test accuracy => 0.728\n",
      "2940\n",
      "test accuracy => 0.724\n",
      "2955\n",
      "test accuracy => 0.724\n",
      "2970\n",
      "test accuracy => 0.728\n",
      "2985\n",
      "test accuracy => 0.728\n",
      "3000\n",
      "test accuracy => 0.722\n",
      "3015\n",
      "test accuracy => 0.728\n",
      "3030\n",
      "test accuracy => 0.724\n",
      "3045\n",
      "test accuracy => 0.705\n",
      "3060\n",
      "test accuracy => 0.726\n",
      "3075\n",
      "test accuracy => 0.726\n",
      "3090\n",
      "test accuracy => 0.724\n",
      "3105\n",
      "test accuracy => 0.703\n",
      "3120\n",
      "test accuracy => 0.724\n",
      "3135\n",
      "test accuracy => 0.692\n",
      "3150\n",
      "test accuracy => 0.720\n",
      "3165\n",
      "test accuracy => 0.728\n",
      "3180\n",
      "test accuracy => 0.720\n",
      "3195\n",
      "test accuracy => 0.729\n",
      "3210\n",
      "test accuracy => 0.716\n",
      "3225\n",
      "test accuracy => 0.724\n",
      "3240\n",
      "test accuracy => 0.724\n",
      "3255\n",
      "test accuracy => 0.729\n",
      "3270\n",
      "test accuracy => 0.718\n",
      "3285\n",
      "test accuracy => 0.720\n",
      "3300\n",
      "test accuracy => 0.729\n",
      "3315\n",
      "test accuracy => 0.726\n",
      "3330\n",
      "test accuracy => 0.724\n",
      "3345\n",
      "test accuracy => 0.720\n",
      "3360\n",
      "test accuracy => 0.728\n",
      "3375\n",
      "test accuracy => 0.731\n",
      "3390\n",
      "test accuracy => 0.724\n",
      "3405\n",
      "test accuracy => 0.696\n",
      "3420\n",
      "test accuracy => 0.729\n",
      "3435\n",
      "test accuracy => 0.724\n",
      "3450\n",
      "test accuracy => 0.729\n",
      "3465\n",
      "test accuracy => 0.726\n",
      "3480\n",
      "test accuracy => 0.726\n",
      "3495\n",
      "test accuracy => 0.728\n",
      "3510\n",
      "test accuracy => 0.720\n",
      "3525\n",
      "test accuracy => 0.733\n",
      "3540\n",
      "test accuracy => 0.724\n",
      "3555\n",
      "test accuracy => 0.729\n",
      "3570\n",
      "test accuracy => 0.722\n",
      "3585\n",
      "test accuracy => 0.722\n",
      "3600\n",
      "test accuracy => 0.735\n",
      "3615\n",
      "test accuracy => 0.733\n",
      "3630\n",
      "test accuracy => 0.713\n",
      "3645\n",
      "test accuracy => 0.700\n",
      "3660\n",
      "test accuracy => 0.728\n",
      "3675\n",
      "test accuracy => 0.720\n",
      "3690\n",
      "test accuracy => 0.716\n",
      "3705\n",
      "test accuracy => 0.716\n",
      "3720\n",
      "test accuracy => 0.715\n",
      "3735\n",
      "test accuracy => 0.715\n",
      "3750\n",
      "test accuracy => 0.718\n",
      "3765\n",
      "test accuracy => 0.716\n",
      "3780\n",
      "test accuracy => 0.720\n",
      "3795\n",
      "test accuracy => 0.729\n",
      "3810\n",
      "test accuracy => 0.718\n",
      "3825\n",
      "test accuracy => 0.728\n",
      "3840\n",
      "test accuracy => 0.718\n",
      "3855\n",
      "test accuracy => 0.722\n",
      "3870\n",
      "test accuracy => 0.728\n",
      "3885\n",
      "test accuracy => 0.724\n",
      "3900\n",
      "test accuracy => 0.731\n",
      "3915\n",
      "test accuracy => 0.722\n",
      "3930\n",
      "test accuracy => 0.720\n",
      "3945\n",
      "test accuracy => 0.718\n",
      "3960\n",
      "test accuracy => 0.718\n",
      "3975\n",
      "test accuracy => 0.728\n",
      "3990\n",
      "test accuracy => 0.718\n",
      "4005\n",
      "test accuracy => 0.715\n",
      "4020\n",
      "test accuracy => 0.715\n",
      "4035\n",
      "test accuracy => 0.715\n",
      "4050\n",
      "test accuracy => 0.718\n",
      "4065\n",
      "test accuracy => 0.716\n",
      "4080\n",
      "test accuracy => 0.720\n",
      "4095\n",
      "test accuracy => 0.726\n",
      "4110\n",
      "test accuracy => 0.722\n",
      "4125\n",
      "test accuracy => 0.720\n",
      "4140\n",
      "test accuracy => 0.722\n",
      "4155\n",
      "test accuracy => 0.722\n",
      "4170\n",
      "test accuracy => 0.720\n",
      "4185\n",
      "test accuracy => 0.718\n",
      "4200\n",
      "test accuracy => 0.718\n",
      "4215\n",
      "test accuracy => 0.722\n",
      "4230\n",
      "test accuracy => 0.716\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4245\n",
      "test accuracy => 0.728\n",
      "4260\n",
      "test accuracy => 0.728\n",
      "4275\n",
      "test accuracy => 0.729\n",
      "4290\n",
      "test accuracy => 0.722\n",
      "4305\n",
      "test accuracy => 0.694\n",
      "4320\n",
      "test accuracy => 0.720\n",
      "4335\n",
      "test accuracy => 0.720\n",
      "4350\n",
      "test accuracy => 0.718\n",
      "4365\n",
      "test accuracy => 0.716\n",
      "4380\n",
      "test accuracy => 0.726\n",
      "4395\n",
      "test accuracy => 0.729\n",
      "4410\n",
      "test accuracy => 0.729\n",
      "4425\n",
      "test accuracy => 0.722\n",
      "4440\n",
      "test accuracy => 0.726\n",
      "4455\n",
      "test accuracy => 0.728\n",
      "4470\n",
      "test accuracy => 0.728\n",
      "4485\n",
      "test accuracy => 0.718\n",
      "4500\n",
      "test accuracy => 0.726\n",
      "4515\n",
      "test accuracy => 0.724\n",
      "4530\n",
      "test accuracy => 0.722\n",
      "4545\n",
      "test accuracy => 0.731\n",
      "4560\n",
      "test accuracy => 0.731\n",
      "4575\n",
      "test accuracy => 0.729\n",
      "4590\n",
      "test accuracy => 0.720\n",
      "4605\n",
      "test accuracy => 0.713\n",
      "4620\n",
      "test accuracy => 0.724\n",
      "4635\n",
      "test accuracy => 0.722\n",
      "4650\n",
      "test accuracy => 0.729\n",
      "4665\n",
      "test accuracy => 0.724\n",
      "4680\n",
      "test accuracy => 0.724\n",
      "4695\n",
      "test accuracy => 0.728\n",
      "4710\n",
      "test accuracy => 0.728\n",
      "4725\n",
      "test accuracy => 0.731\n",
      "4740\n",
      "test accuracy => 0.729\n",
      "4755\n",
      "test accuracy => 0.728\n",
      "4770\n",
      "test accuracy => 0.731\n",
      "4785\n",
      "test accuracy => 0.728\n",
      "4800\n",
      "test accuracy => 0.728\n",
      "4815\n",
      "test accuracy => 0.724\n",
      "4830\n",
      "test accuracy => 0.720\n",
      "4845\n",
      "test accuracy => 0.722\n",
      "4860\n",
      "test accuracy => 0.729\n",
      "4875\n",
      "test accuracy => 0.731\n",
      "4890\n",
      "test accuracy => 0.729\n",
      "4905\n",
      "test accuracy => 0.716\n",
      "4920\n",
      "test accuracy => 0.733\n",
      "4935\n",
      "test accuracy => 0.724\n",
      "4950\n",
      "test accuracy => 0.726\n",
      "4965\n",
      "test accuracy => 0.724\n",
      "4980\n",
      "test accuracy => 0.716\n",
      "4995\n",
      "test accuracy => 0.718\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "if (flag_use_sentiment_for_word):\n",
    "    embedding_size = embedding_size + nb_sentiment_for_word\n",
    "\n",
    "model = Model(batch_size, seq_max_len, nb_sentiment_label, nb_sentiment_for_word, embedding_size, nb_linear_inside, nb_lstm_inside, layers, TRAINING_ITERATIONS, LEARNING_RATE, WEIGHT_DECAY, flag_train, flag_use_sentiment_for_word,sess)\n",
    "\n",
    "model.modeling()\n",
    "model.train(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predictRAW(model, data):\n",
    "    prediction_test = model.sess.run(model.prediction, \n",
    "                      feed_dict={model.tf_X_train: np.asarray(data.x_test),\n",
    "                                 model.tf_X_binary_mask: np.asarray(data.test_binary_mask),\n",
    "                                 model.tf_X_seq_len: np.asarray(data.test_seq_len),\n",
    "                                 model.tf_X_sent_for_word: np.asarray(data.test_sentiment_for_word),\n",
    "                                 model.keep_prob: 1.0})\n",
    "\n",
    "\n",
    "    ret = list()\n",
    "    for i in range(len(data.test_data)):\n",
    "            data_sample = ''\n",
    "            for j in range(len(data.test_data[i])):\n",
    "                if data.word_dict_rev[data.test_data[i][j]] == '<unk>':\n",
    "                    continue\n",
    "                elif data.test_binary_mask[i][j] > 0.:\n",
    "                    data_sample = data_sample + data.word_dict_rev[data.test_data[i][j]] + \\\n",
    "                     '(predict ' + str(prediction_test[i][j]) + ') '\n",
    "                else:\n",
    "                    data_sample = data_sample + data.word_dict_rev[data.test_data[i][j]] + ' '\n",
    "            ret.append(data_sample.replace('<padding>', '').strip())\n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictRAW(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
